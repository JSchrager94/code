{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is only one extra step: you need to transform the array of inputs to include non-linear terms such as ùë•¬≤.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now you have the input and output in a suitable format. \n",
    "#Keep in mind that you need the input to be a two-dimensional array. That‚Äôs why .reshape() is used.\n",
    "\n",
    "\n",
    "x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\n",
    "y = np.array([15, 11, 2, 8, 25, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the new step you need to implement for polynomial regression!\n",
    "\n",
    "#As you‚Äôve seen earlier, you need to include ùë•¬≤ (and perhaps other terms) as additional features when implementing polynomial regression. For that reason, you should transform the input array x to contain the additional column(s) with the values of ùë•¬≤ (and eventually more features).\n",
    "\n",
    "#It‚Äôs possible to transform the input array in several ways (like using insert() from numpy), but the class PolynomialFeatures is very convenient for this purpose. Let‚Äôs create an instance of this class:\n",
    "\n",
    "transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "#The variable transformer refers to an instance of PolynomialFeatures which you can use to transform the input x.\n",
    "\n",
    "#You can provide several optional parameters to PolynomialFeatures:\n",
    "\n",
    "#degree is an integer (2 by default) that represents the degree of the polynomial regression function.\n",
    "#interaction_only is a Boolean (False by default) that decides whether to include only interaction features (True) or all features (False).\n",
    "#include_bias is a Boolean (True by default) that decides whether to include the bias (intercept) column of ones (True) or not (False).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolynomialFeatures(include_bias=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This example uses the default values of all parameters, but you‚Äôll sometimes want to experiment with the degree of the function, and it can be beneficial to provide this argument anyway.\n",
    "\n",
    "#Before applying transformer, you need to fit it with .fit():\n",
    "\n",
    "transformer.fit(x)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once transformer is fitted, it‚Äôs ready to create a new, modified input. You apply .transform() to do that:\n",
    "x_ = transformer.transform(x)\n",
    "\n",
    "#That‚Äôs the transformation of the input array with .transform(). It takes the input array as the argument and returns the modified array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can also use .fit_transform() to replace the three previous statements with only one:\n",
    "x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   5.   25.]\n",
      " [  15.  225.]\n",
      " [  25.  625.]\n",
      " [  35. 1225.]\n",
      " [  45. 2025.]\n",
      " [  55. 3025.]]\n"
     ]
    }
   ],
   "source": [
    "#That‚Äôs fitting and transforming the input array in one statement with .fit_transform(). \n",
    "#It also takes the input array and effectively does the same thing as .fit() and .transform() called in that order. \n",
    "#It also returns the modified array. This is how the new input array looks:\n",
    "\n",
    "print(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(x_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.8908516262498563\n",
      "intercept: 21.372321428571418\n",
      "coefficients: [-1.32357143  0.02839286]\n"
     ]
    }
   ],
   "source": [
    "#Again, .score() returns ùëÖ¬≤. Its first argument is also the modified input x_, not x. \n",
    "#The values of the weights are associated to .intercept_ and .coef_: .intercept_ represents ùëè‚ÇÄ, while .coef_ references the array that contains ùëè‚ÇÅ and ùëè‚ÇÇ respectively.\n",
    "\n",
    "\n",
    "\n",
    "r_sq = model.score(x_, y)\n",
    "print('coefficient of determination:', r_sq)\n",
    "print('intercept:', model.intercept_)\n",
    "print('coefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can obtain a very similar result with different transformation and regression arguments:\n",
    "\n",
    "x_ = PolynomialFeatures(degree=2, include_bias=True).fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 5.000e+00 2.500e+01]\n",
      " [1.000e+00 1.500e+01 2.250e+02]\n",
      " [1.000e+00 2.500e+01 6.250e+02]\n",
      " [1.000e+00 3.500e+01 1.225e+03]\n",
      " [1.000e+00 4.500e+01 2.025e+03]\n",
      " [1.000e+00 5.500e+01 3.025e+03]]\n"
     ]
    }
   ],
   "source": [
    "#This is how the modified input array looks in this case:\n",
    "\n",
    "print(x_)\n",
    "\n",
    "#The first column of x_ contains ones, the second has the values of x, while the third holds the squares of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The intercept is already included with the leftmost column of ones, and you don‚Äôt need to include it again \n",
    "#when creating the instance of LinearRegression. Thus, you can provide fit_intercept=False. \n",
    "#This is how the next statement looks:\n",
    "\n",
    "model = LinearRegression(fit_intercept=False).fit(x_, y)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.8908516262498565\n",
      "intercept: 0.0\n",
      "coefficients: [21.37232143 -1.32357143  0.02839286]\n"
     ]
    }
   ],
   "source": [
    "r_sq = model.score(x_, y)\n",
    "print('coefficient of determination:', r_sq)\n",
    "print('intercept:', model.intercept_)\n",
    "print('coefficients:', model.coef_)\n",
    "\n",
    "#ou see that now .intercept_ is zero, but .coef_ actually contains ùëè‚ÇÄ as its first element. Everything else is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted response:\n",
      "[15.46428571  7.90714286  6.02857143  9.82857143 19.30714286 34.46428571]\n"
     ]
    }
   ],
   "source": [
    "#If you want to get the predicted response, just use .predict(), but remember that the argument should be the modified input x_ instead of the old x:\n",
    "\n",
    "y_pred = model.predict(x_)\n",
    "print('predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can apply the identical procedure if you have several input variables. You‚Äôll have an input array with more than one column, but everything else is the same. Here is an example:\n",
    "\n",
    "# Step 1: Import packages\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Step 2a: Provide data\n",
    "x = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]]\n",
    "y = [4, 5, 20, 14, 32, 22, 38, 43]\n",
    "x, y = np.array(x), np.array(y)\n",
    "\n",
    "# Step 2b: Transform input data\n",
    "x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)\n",
    "\n",
    "# Step 3: Create a model and fit it\n",
    "model = LinearRegression().fit(x_, y)\n",
    "\n",
    "# Step 4: Get results\n",
    "r_sq = model.score(x_, y)\n",
    "intercept, coefficients = model.intercept_, model.coef_\n",
    "\n",
    "# Step 5: Predict\n",
    "y_pred = model.predict(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.9453701449127819\n",
      "intercept: 0.8430556452395876\n",
      "coefficients:\n",
      "[ 2.44828275  0.16160353 -0.15259677  0.47928683 -0.4641851 ]\n",
      "predicted response:\n",
      "[ 0.54047408 11.36340283 16.07809622 15.79139    29.73858619 23.50834636\n",
      " 39.05631386 41.92339046]\n"
     ]
    }
   ],
   "source": [
    "print('coefficient of determination:', r_sq)\n",
    "\n",
    "print('intercept:', intercept)\n",
    "\n",
    "print('coefficients:', coefficients, sep='\\n')\n",
    "\n",
    "print('predicted response:', y_pred, sep='\\n')\n",
    "\n",
    "\n",
    "#In this case, there are six regression coefficients (including the intercept), as shown in the estimated regression function ùëì(ùë•‚ÇÅ, ùë•‚ÇÇ) = ùëè‚ÇÄ + ùëè‚ÇÅùë•‚ÇÅ + ùëè‚ÇÇùë•‚ÇÇ + ùëè‚ÇÉùë•‚ÇÅ¬≤ + ùëè‚ÇÑùë•‚ÇÅùë•‚ÇÇ + ùëè‚ÇÖùë•‚ÇÇ¬≤.\n",
    "\n",
    "#You can also notice that polynomial regression yielded a higher coefficient of determination than multiple linear regression for the same problem. At first, you could think that obtaining such a large ùëÖ¬≤ is an excellent result. It might be.\n",
    "\n",
    "#However, in real-world situations, having a complex model and ùëÖ¬≤ very close to 1 might also be a sign of overfitting. To check the performance of a model, you should test it with new data, that is with observations not used to fit (train) the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
